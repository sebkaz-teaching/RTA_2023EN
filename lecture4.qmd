---
title: "Lecture 4 - Streaming architecture"
---

A streaming architecture is a defined set of technologies that work together to handle stream processing, which is the practice of taking action on a series of data at the time the data is created.
In many modern deployments, Apache Kafka acts as the store for the streaming data, and then multiple stream processors can act on the data stored in Kafka to produce multiple outputs. 
Some streaming architectures include workflows for both stream processing and batch processing, which either entails other technologies to handle large-scale batch processing, or using Kafka as the central store as specified in the Kappa Architecture.

An excellent real-time data processing architecture needs to be fault-tolerant and scalable; it needs to support batch and incremental updates and be extensible.

At the begining we explore two essential data processing architectures known as `Lambda` and `Kappa` that serve as the backbone of various enterprise application.


## Lambda architecture

Lambda architecture comprises: `Batch Layer`, `Speed Layer` (also known as Stream layer), and `Serving Layer`.

_The batch layer_ operates on the complete data and thus allows the system to produce the most accurate results. 
However, the results come at the cost of high latency due to high computation time. 
The batch layer stores the raw data as it arrives and computes the batch views for consumption. 
Naturally, batch processes will occur at some interval and will be long-lived. The scope of data is anywhere from hours to years.

_The speed layer_ generates results in a low-latency, near real-time fashion.
The speed layer is used to compute the real-time views to complement the batch views. 
The speed layer receives the arriving data and performs incremental updates to the batch layer results. 
Thanks to the incremental algorithms implemented at the speed layer, the computation cost is significantly reduced.

The batch views may be processed with more complex or expensive rules and may have better data quality and less skew, while the real-time views give you up-to-the-moment access to the latest possible data.

Finally, _the serving layer_ enables various queries of the results sent from the batch and speed layers. 
The outputs from the batch layer in the form of batch views and the speed layer in the form of near-real-time views are forwarded to the serving layer, which uses this data to cater to the pending queries on an ad-hoc basis.

</br>
<img src = "img/lambda_arch.png" class="center" />

Implementation Example: 
</br>
<img src = "img/lambda_app.png" class="center" />

**Good**

1. Good balance of speed, reliability, and scalability. 
2. Access to both real-time and offline results in covering many data analysis scenarios very well. 
3. Having access to a complete data set in a batch window may yield specific optimizations that make Lambda better performing and perhaps even simpler to implement.


**Bad**

1. Internal processing logic is the same (batch an real-time layers) - many duplicate modules and coding. 
2. A data set modeled with Lambda architecture is difficult to migrate and reorganize. 

## Kappa architecture

The Kappa Architecture is a software architecture used for processing streaming data. 
The main premise behind the Kappa Architecture is that you can perform both real-time and batch processing, especially for analytics, with a single technology stack. 
It is based on a streaming architecture in which an incoming series of data is first stored in a messaging engine like Apache Kafka. 
From there, a stream processing engine will read the data and transform it into an analyzable format, and then store it into an analytics database for end users to query.

The Kappa Architecture supports (near) real-time analytics when the data is read and transformed immediately after it is inserted into the messaging engine. 
This makes recent data quickly available for end user queries. 
It also supports historical analytics by reading the stored streaming data from the messaging engine at a later time in a batch manner, to create additional analyzable outputs for more types of analysis.

The Kappa Architecture is considered a simpler alternative to the Lambda Architecture as it uses the same technology stack to handle both real-time stream processing and historical batch processing. 
Both architectures entail the storage of historical data to enable large-scale analytics. 
Both architectures are also useful for addressing “human fault tolerance,” in which problems with the processing code (either bugs or just known limitations) can be overcome by updating the code and running it again on the historical data. 
The main difference with the Kappa Architecture is that all data is treated as if it were a stream, so the stream processing engine acts as the sole data transformation engine.

</br>
<img src = "img/kappa1.png" class="center" />

Implementation Example: 
</br>
<img src = "img/kappa2.png" class="center" />

### Good 

1. Applications can read and write directly to Kafka as developed. For existing event sources, listaners are used to stream writes directly from database logs eliminating the need for batch processing during ingress, resulting in fewer resources.
2. Queries only need to look in a single serving lcation instead of going against batch and speed viwes. 

### Bad

1. not easy to implement, especially for the data replay.
2. 


### How do the Lambda and Kappa compare? 

Both architectures handle real-time and historical analytics in a single environment. 
However, one major benefit of the Kappa Architecture over the Lambda Architecture is that it enables you to build your streaming and batch processing system on a single technology. 
This means you can build a stream processing application to handle real-time data, and if you need to modify your output, you update your code and then run it again over the data in the messaging engine in a batch manner. 
There is no separate technology to handle the batch processing, as is suggested by the Lambda Architecture.

With a sufficiently fast stream processing engine, you may not need a separate technology that is optimized for batch processing. 
You simply read the stored streaming data in parallel (assuming the data in Kafka is appropriately split into separate channels, or “partitions”) and transform the data as if it were from a streaming source.
For some environments, you can potentially create the analyzable output on demand, so when a new query is submitted from an end user, the data can be transformed ad hoc to optimally answer that query.
Again, this requires a high-speed stream processing engine to enable low latency in the processing.

While the Lambda Architecture does not specify the technologies that must be used, the batch processing component is often done on a large-scale data platform like Apache Hadoop. 
_The Hadoop Distributed File System_ (HDFS) can economically store the `raw data` that can then be transformed via Hadoop tools into an analyzable format. 
While Hadoop is used for the batch processing component of the system, a separate engine designed for stream processing is used for the real-time analytics component. 
One advantage of the Lambda Architecture, however, is that much larger data sets (in the petabyte range) can be stored and processed more efficiently in Hadoop for large-scale historical analysis.

## Publish/Subscribe

`Publish/Subscribe` messaging system is a critical component of data-driven apps. _Pub/Sub messaging_ is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specificially directing it to a receiver. 
Pub/sub systems often have a broker, a central point where messages.

## Apache Kafka

On [Kafka website](https://kafka.apache.org), you'll find the definition of it right on the first page: 

> A distributed streaming platform

**What is an “A distributed streaming platform”?**

First, I would like to remind you what a `stream` is.
Streams are just unlimited data, data that never end. 
It just keeps arriving, and you can process it in real time.

And `distributed`? Distributed means that Kafka works in a cluster, each node in the cluster is called Broker. 
Those brokers are just servers executing a copy of apache Kafka.

So, basically, Kafka is a set of machines working together to be able to handle and process real-time infinite data.

His distributed architecture is one of the reasons that made Kafka so popular.
The Brokers is what makes it so resilient, reliable, scalable, and fault-tolerant. 
That’s why Kafka is so performer and secure.
But why there is this misconception that Kafka is just another `messaging system`?

To respond to that answer, we need first to explain how messaging works.

### Messaging system

Messaging, very briefly, it’s just the act to send a message from one place to another. It has three principal actors:

- Producer: Who produces and send the messages to one or more queues;
- Queue: A buffer data structure, that receives (from the producers) and delivers messages (to the consumers) in a FIFO (First-In First-Out) way. When a message is delivered, it’s removed forever from the queue, there’s no chance to get it back;
- Consumer: Who is subscribed to one or more queues and receives their messages when published.

And that is it, this is how the messaging works (very briefly, there’s a lot more). As you can see there’s nothing about streams, real-time, cluster (depending on what tool you chose, you can use a cluster too, but it’s not native, like Kafka).

### Kafka architecture

A lot of information You can find in this [link](https://www.cloudkarafka.com/blog/part1-kafka-for-beginners-what-is-apache-kafka.html).

Now that we know how messaging works, let’s get dive into the Kafka world. 
In Kafka, we also have the `Producers` and `Consumers`, they work in a very similar way that they work in the messaging, both producing and consuming messages.

</br>
<img src="img/kafka1.png" class="center" />


As you can see it is very similar to what we’ve discussed about messaging, but here we don’t have the `Queue` concept, instead, we have the concept of `Topics`.

The `Topic` is a very specific type of data stream, it’s very similar to a Queue, it receives and delivers messages as well, but there are some concepts that we need to understand about topics:

- A topic is divided into `partitions`, each topic can have one or more partitions and we need to specify that number when we're creating the topic. 
You can imagine the topic as a folder in the operation system and each folder inside her as a partition.
<!-- - If we don’t give any key to a message when we’re producing it, by default, the producers will send the message in a round-robin way, each partition will receive a message (even if they are sent by the same producer). 
Because of that, we aren’t able to guarantee the delivery order at the partition level, if we want to send a message always to the same partition, we need to give a key to our messages. This will ensure that the message will always be sent to the same partition; -->
- Each message will be stored in the broker disk and will receive an offset (unique identifier). This offset is unique at the partition level, each partition has its owns offsets. That is one more reason that makes Kafka so special, it stores the messages in the disk (like a database, and in fact, Kafka is a database too) to be able to recover them later if necessary. Different from a messaging system, that the message is deleted after being consumed;
- The producers use the offset to read the messages, they read from the oldest to the newest. In case of consumer failure, when it recovers, will start reading from the last offset.
</br>
<img src="img/kafka2.png" class="center" />

### Brokers

As said before, Kafka works in a distributed way. A Kafka cluster may contain many brokers as needed.

</br>
<img src="img/kafka3.png" class="center" />

Each broker in a cluster is identified by an ID and contains at least one partition of a topic. 
To configure the number of the partitions in each broker, we need to configure something called Replication Factor when creating a topic. 
Let’s say that we have three brokers in our cluster, a topic with three partitions and a Replication Factor of three, in that case, each broker will be responsible for one partition of the topic.

As you can see in the above image, $Topic_1$ has three partitions, each broker is responsible for a partition of the topic, so, the Replication Factor of the $Topic_1$ is three.
It’s very important that the number of the partitions match the number of the brokers, in this way, each broker will be responsible for a single partition of the topic.

### Producers

Just like in the messaging world, `Producers` in Kafka are the ones who produce and send the messages to the topics.
As said before, the messages are sent in a round-robin way.
Ex: Message 01 goes to partition 0 of Topic 1, and message 02 to partition 1 of the same topic. It means that we can’t guarantee that messages produced by the same producer will always be delivered to the same topic.
We need to specify a key when sending the message, Kafka will generate a hash based on that key and will know what partition to deliver that message.
That hash takes into consideration the number of the partitions of the topic, that’s why that number cannot be changed when the topic is already created.

<!-- When we are working with the concept of messages, there’s something called Acknowledgment (ack). The ack is basically a confirmation that the message was delivered. In Kafka, we can configure this ack when producing the messages. There are three different levels of configuration for that:
ack = 0: When we configure the ack = 0, we’re saying that we don’t want to receive the ack from Kafka. In case of broker failure, the message will be lost;
ack = 1: This is the default configuration, with that we’re saying that we want to receive an ack from the leader of the partition. The data will only be lost if the leader goes down (still there’s a chance);
ack = all: This is the most reliable configuration. We are saying that we want to not only receive a confirmation from the leader but from their replicas as well. This is the most secure configuration since there’s no data loss. Remembering that the replicas need to be in-sync (ISR). If a single replica isn’t, Kafka will wait for the sync to send back de ack. -->

### Consumers and Consumers Groups

`Consumers` are applications subscribed to one or more topics that will read messages from there. 
They can read from one or more partitions.
When a consumer reads from just one partition, we can ensure the order of the reading, but when a single consumer reads from two or more partitions, it will read in parallel, so, there’s no guarantee of the reading order. 
A message that came later can be read before another that came earlier, for example.
That’s why we need to be careful when choosing the number of partitions and when producing the messages.

Another important concept of Kafka is the `Consumer Groups`. 
It’s really important when we need to scale the messages reading.
It becomes very costly when a single consumer needs to read from many partitions, so, we need o load-balancing this charge between our consumers, this is when the consumer groups enter.

The data from a single topic will be load-balancing between the consumers, with that, we can guarantee that our consumers will be able to handle and process the data.
The ideal is to have the same number of consumers in a group that we have as partitions in a topic, in this way, every consumer read from only one. 
When adding consumers to a group, you need to be careful, if the number of consumers is greater than the number of partitions, some consumers will not read from any topic and will stay idle.


</br>
<img src="img/kafka4.png" class="center" />