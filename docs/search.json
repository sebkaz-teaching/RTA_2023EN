[
  {
    "objectID": "lecture1.html",
    "href": "lecture1.html",
    "title": "Lecture 1 Small data",
    "section": "",
    "text": "The stream processing technology is becoming more and more popular with big and small companies because it provides superior solutions for many established use cases such as data analytics, ETL, transactional apps, software architectures, and business opportunities. We try to describe why stateful stream processing is becoming so popular and assess its potential.\nBut first, we start by reviewing classical data app architectures and point out their limitations."
  },
  {
    "objectID": "lecture1.html#data---small-data--",
    "href": "lecture1.html#data---small-data--",
    "title": "Lecture 1 Small data",
    "section": "Data -> Small Data -> …",
    "text": "Data -> Small Data -> …\nThe development of information technology has resulted in access to unimaginable amounts of a new resources which are structured and unstructured data.\nData has contributed to the development of thousands of new tools for generating, collecting, storing and processing information on an unprecedented scale.\n\n\nThe emergence of new scientific or business challenges becomes possible thanks to the construction of systems based on open software, and the use of home computers to support the processing of huge amounts of data.\nThe new kind of business and scientific challenges include:\n\nintelligent advertising of thousands of products for millions of customers,\nprocessing of data about genes, RNA or proteins genus,\nintelligent detection of various methods of fraud among hundreds of billions of credit card transactions,\nstock market simulations based on thousands of financial instruments,\n… The data age presents us with newer and newer challenges related not only to the quantity but also to the time of data processing.\n\nAll machine learning algorithms require structured data written in a tabular form. They are organized in columns of characteristics that characterize each observation (rows). An example may be such features as sex, growth or the number of owned cars, of which it can be predicted whether the customer will repay the loan or not. This prediction is also collected as a feature. Thanks to the tables of features obtained in this way, we can use XGBoost or logistic regression algorithms to determine the appropriate combination of variables affecting the probability of a good or bad customer.\nUnstructured data is data that is not arranged in a tabular form. Examples include sound, images and text. In the process of processing, they are always converted into some vector form. However, individual letters, frequencies or pixels do not convey any information. They do not create separate features, which is crucial to distinguish them from structured data.\n\nGive an example of structured and unstructured data. Load sample data in jupyter notebook.\n\n\nKnows the types of structured and unstructured data (K2A_W02, K2A_W04, O2_W04, O2_W07)"
  },
  {
    "objectID": "lecture1.html#data-sources",
    "href": "lecture1.html#data-sources",
    "title": "Lecture 1 Small data",
    "section": "Data sources",
    "text": "Data sources\nThe three largest data generators are:\n\nsocial data in the form of texts (tweets, entries in social networks, comments), photos or videos. These data are very important due to their wide possibilities of consumer behaviour and sentiment analysis in marketing analyses.\ndata from all kinds of sensors or logs of the operation of devices and users (e.g. on a website). These data are related to IoT (Internet of Things) technology, which is currently one of the most developing areas in data processing, but also in the business direction.\nTransaction data, which is generally what is always generated as transactions appearing both online and offline. Currently, this type of data is processed for the purpose of performing transactions and rich analytics supporting virtually every area of ​​everyday life."
  },
  {
    "objectID": "lecture1.html#actual-data-generation-process",
    "href": "lecture1.html#actual-data-generation-process",
    "title": "Lecture 1 Small data",
    "section": "Actual data generation process",
    "text": "Actual data generation process\nThe data that is in reality appears as a result of the continuous operation of the systems. You have generated a lot of data on your phone today (and even on these devices!) Will it not generate them early or tomorrow? Batch processing splits the data into a time-length chunk and runs granular processes at a user-specified time . However, the timestamp is not always appropriate.\nWith many systems that handle the data streams that you already have. They are e.g.: - data warehouses - devices monitoring systems (IoT) - transaction systems - website analytics systems - Internet advertising - social media - operating systems - ….\n\na company is an organization that works and responds to a constant stream of data.\n\nThe input to the orchard source (but also the result of the evaluation) of the data is the file. It is written once and can be referred to (multiple functions - tasks can run on it). The name of the file to identify the record set.\nIn the case of the stream of change, it is only once through the so-called manufacturer (also referred to as the sender or supplier). They can be formed by many so-called consumers (recipients). Streaming events are grouped into so-called topic (eng. topic)."
  },
  {
    "objectID": "lecture1.html#not-to-big-data",
    "href": "lecture1.html#not-to-big-data",
    "title": "Lecture 1 Small data",
    "section": "not to Big Data",
    "text": "not to Big Data\n\n,,Big Data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so every one claims they are doing it.’’ — Dan Ariely, Professor of Psychology and Behavioral Economics, Duke University\n\n\none, two, … four V\n\nVolume - the size of the data produced worldwide is growing exponentially. Huge amounts of data are being generated every second - the email you send, Twitter, Facebook, or other social media, videos, pictures, SMS messages, call records and data from varied devices and sensors.\nVelocity - the speed of data production, the speed of their transfer and processing.\nVariety - we associate traditional data with an alphanumeric form composed of letters and numbers. Currently, we have images, sounds, videos and IoT data streams at our disposal\nVeracity - Is the data complete and correct? Do they objectively reflect reality? Are they the basis for making decisions?\nValue - The value that the data holds. In the end, it’s all about cost and benefits.\n\n\nThe purpose of calculations is not numbers, but understanding them R.W. Hamming 1962.\n\nAs You can see data and data processing have been omnipresent in businesses for many decades. Over the years the collection and usage of data have grown consistently, and companies have designed and built infrastructures to manage that data."
  },
  {
    "objectID": "lecture1.html#data-processing-models",
    "href": "lecture1.html#data-processing-models",
    "title": "Lecture 1 Small data",
    "section": "Data processing models",
    "text": "Data processing models\nThe traditional architecture that most businesses implement distinguishes two types of data processing.\nMost of the data is stored in databases or data warehouses. By default, access to data comes down to the implementation of queries via applications. The method of using and implementing the database access process is called the processing model. Two implementations are most commonly used:\n\nTraditional Model\nTraditional model - on-line transaction processing, OLTP (on-line transaction processing). It works great in the case of ongoing service, e.g. customer service, order register, sales service, etc. Companies use all kinds of applications for their day-to-day business activities, such as Enterprise Resource Planning (ERP) Systems, Customer Relationship Management (CRM) software, and web-based applications. These systems are typically designed with separate tiers for data processing and data storage (transactional database system).\n\n\nApplications are usually connected to external services or face human users and continuously process incoming events such as orders, emails, or clicks on a website.\nWhen an event is processed, an application reads its state or updates it by running transactions against the remote database system. Often, a database system serves multiple applications that sometimes access the same databases or tables.\nThis model provides effective solutions for:\n\neffective and safe data storage,\ntransactional data recovery after a failure,\ndata access optimization,\nconcurrency management,\nevent processing -> read -> write\n\nAnd what if we are dealing with:\n\naggregation of data from many systems (e.g. for many stores),\nsupporting data analysis,\ndata reporting and summaries,\noptimization of complex queries,\nsupporting business decisions.\n\nResearch on such issues has led to the formulation of a new data processing model and a new type of database (Data warehouse).\nThis application design can cause problems when applications need to evolve or scale. Since multiple applications might work on the same data representation or share the same infrastructure, changing the schema of a table or scaling a database system requires careful planning and a lot of effort. Currently, many running applications (even in one area) are implemented as microservices, i.e. small and independent applications (LINUX programming philosophy - do little but right). Because microservices are strictly decoupled from each other and only communicate over well-defined interfaces, each microservice can be implemented with a different technology stack including a programming language, libraries and data stores.\nThis model provides effective solutions for:\n\neffective and safe data storage,\ntransactional data recovery after a failure,\ndata access optimization,\nconcurrency management,\nevent processing -> read -> write\n\nAnd what if we are dealing with:\n\naggregation of data from many systems (e.g. for many stores),\nsupporting data analysis,\ndata reporting and summaries,\noptimization of complex queries,\nsupporting business decisions.\n\nResearch on such issues has led to the formulation of a new data processing model and a new type of database (Data warehouse).\nThis application design can cause problems when applications need to evolve or scale. Since multiple applications might work on the same data representation or share the same infrastructure, changing the schema of a table or scaling a database system requires careful planning and a lot of effort. Currently, many running applications (even in one area) are implemented as microservices, i.e. small and independent applications (LINUX programming philosophy - do little but right). Because microservices are strictly decoupled from each other and only communicate over well-defined interfaces, each microservice can be implemented with a different technology stack including a programming language, libraries and data stores.\nBoth are performed in batch mode. Today they are strictly made using Hadoop technology."
  },
  {
    "objectID": "lecture2.html",
    "href": "lecture2.html",
    "title": "Lecture 2 - Time to Stream",
    "section": "",
    "text": "Expectations vs Reality\n\n\nWhen to take a business decision?\n\n\n\n\n\nBatch = Big, historical datasets\nStream = stream data, online, generated and send continuously\n\n\n\n\n\nBatch = minutes, hours, days (Data warehouses)\nStream = Real-time/near-real-time\n\n\n\n\n\nBatch = possible and used very often\nStream = ,,impossible’’\n\n\n\n\n\nExtract, Transform, Load is a basic pattern for data processing, commonly known in data warehousing. It’s all about extracting data from a source, transforming the data (business rules) and at the end writing/loading everything to a target (Hadoop, Relational Database, Data Warehouse etc.)"
  },
  {
    "objectID": "lecture2.html#big-data",
    "href": "lecture2.html#big-data",
    "title": "Lecture 2 - Time to Stream",
    "section": "Big Data",
    "text": "Big Data\nBig Data system can be a part of (source of) data warehouses (ex. Data Lake, Enterprise Data Hub)\nBut Data Warehouses are not Big Data Systems!\n\nData warehouses\n\n\nhighly structured data retention\nfocused on the analysis and reporting process\n100 % accuracy\n\n\nBig Data\n\n\ndata of any structure\nserves a variety of data-driven purposes (analytics, data science …)\nless than 100 % accuracy\n\n\n\nELT process\nELT process is similar to ETL and has the same stages involved, but the order of performing ETL stages is different. Extract data from one or many sources and Load it to the destination system for example “data lake”. After that, You can Transform Your data a more dynamically on demand.\nUse Case: - Demanding scalability requirements of Big Data - Streaming analytics - Integration of highly distributed data sources - Multiple data products from the same sources\nELT is an emerging trend: - Big Data -> Cloud computing - ELT separates the data pipeline from processing - More flexibility - No information loss (by transformations)\nDifference between ETL and ELT\n\nTransformations for ETL happen within the data pipeline\nTransformations for ELT happen in the destination environment\nETL is rigid - pipelines are engineered to user specifications\nELT is flexible - end users build their transformations\nOrganizations use ETL for relational data, on-premise - scalability is difficult\nELT solves scalability problems, handling both structured and unstructured Big Data in the cloud\nETL workflows take time to specify and develop\nELT supports self-serve, interactive analytics in real time\n\nELT is the evolution of ETL! - increasing demand for access to raw data.\nETL still has its place for many applications: - Lengthy time-to-insight - Challenges imposed by Big Data - Demand for access to siloed information\n\n\nData Extraction Techniques\nExamples of raw data sources: - Paper documents - Web pages - Analog audio/video - Survey, statistics, economics - Transactional data - Social media - Weather station networks - IoT - Medical records - Human genomes\nData extraction techniques include: - OCR - ADC sampling, CCD sampling - Mail, phone, or in-person surveys and polls - Cookies, user logs - Web scraping - API’s - Database querying - Edge computing - Biomedical devices\n\n\nData Transformation Techniques\ncan involve various operations, such as: - Data typing - Data structuring - Anonymizing, encrypting - Cleaning: duplicate records, missing values - Normalizing: converting data to common units - Filtering, sorting, aggregating, binning - Joining data sources\n\nSchema-on-write\nis the conventional ETL approach: - Consistency and efficiency - Limited versatility\n\n\nSchema-on-read\napplies to the modern ELT approach: - Versatility - Enhanced storage flexibility = more data\nTransformed data could be loss information. - data compression - filtering - aggregation - Edge computing devices\n\n\n\nData Load Techniques\n\nFull\nIncremental\nScheduled\nON-demand\nBatch and stream\npush and pull\nparallel and serial"
  },
  {
    "objectID": "lecture2.html#hadoop-map-reduce",
    "href": "lecture2.html#hadoop-map-reduce",
    "title": "Lecture 2 - Time to Stream",
    "section": "Hadoop Map-Reduce",
    "text": "Hadoop Map-Reduce\n\n\nFind a simple map-reduce algorithm in any programming language and run it.\n\n\nHow to improve?\n\nAPACHE SPARK"
  }
]